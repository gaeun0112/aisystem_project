# 정리(정연)

### Abstract

- 왜 이 실험을 하고 싶었는지

원래는 RNN 파트를 맡았었음. 그런데 RNN 아키텍처는 h에 정보를 누적시킨다는 아이디어 때문에 발산형 개형의 활성화함수를 쓸 수 없음. 때문에 RNN 구조에 대해 논의되는 다양한 활성화함수를 적용 시켜보는 것의 의의점을  찾지 못함.

관념적으로 렐루 혹은 리키렐루를 현재 우리는 활성화함수로 쓰고 있는데 왜 이런 개형의 활성화함수가 제안되어 자리잡았는지 그 배경을 이해하고, 하이퍼파라미터인 활성화함수의 종류를 학습파라미터로 둘 수 있지 않을까 하는 호기심에서 실험이 시작됨.

### Background & Related Works

- 선행연구

선형연구1) 선행논문정리

 보통 ANN은 각 뉴런에서 고정된, 비선형 활성화 함수를 사용한다. 하지만 본 논문은 경사하강법을 사용해 각 뉴런에서 독립적으로 학습된 조각난-선형함수를 디자인해, 적절한 활성화함수 개형을 학습해 나가는 아이디어를 사용한다.

선형함수의 파라미터는 인풋데이터에 fit되도록 학습되는 반면, 비선형함수를 사용할 경우 (로지스틱, tanh, rectified linear, max-pooling function) 중에서 미리 결정하고 학습을 시작한다.

(Hornik et al., 1989; Cho & Saul, 2010) 논문에서는 이렇게 비선형함수 중 하나를 사용해 충분히 큰 신경망을 구성할 경우, 임의의 복잡한 함수를 근사할 수 있음을 밝혔다.

 활성화함수 사용 1차 시대 - 비선형 활성화 함수 사용 (선형함수는 층 쌓아도 의미가 없음)

(Jarrett et al., 2009; Glorot et al., 2011) 논문에서는 전통적인 시그모이드 대신 Relu(saturate되지 않은 rectified 선형 활성화함수) 활성화함수 사용의 장점을 밝혔다. (그래디언트소실문제완화 + Deep 신경망의 학습 속도 up)

 활성화함수 사용 2차 시대 - rectified 선형 활성화함수 사용 - 사용의 측면에서 현재에 해당함

*saturate : 시그모이드는 양 끝에서 0 또는 1에 수렴하는데, 입력이 매우 크거나 작을 때 극단적인 값으로 수렴되는 현상 (시그모이드는 아무리 커져도 입력값 그대로를 반환)

*rectified = 개선된 = 인공지능학계 사이에서는 relu를 일컫는 말

*relu = Rectified Linear Unit

 활성화함수 사용 2.5차 시대 - 렐루를 보완하기 위한 다양한 활성화함수 탐구 + 다른 활성화 함수 제안

리키렐루를 비롯해 렐루의 변형적 측면에서 더 다양한 활성화함수들이 탐구되었지만, 연산량과 일반화 측면에서 받아들여지지 못함.

 활성화함수 사용 3차 시대 - 적응형 활성화함수 사용 고민 - 실험의 측면에서 현재!

고정된 활성화함수 쓰는 것 보다 더 나은 방법 없을까에 대한 고민

즉, 훈련 과정에서 활성화함수를 학습하자는 것!

이를 위한 이전 노력에는 **미리 정의된 집합에서** 각 뉴런을 위한 활성화함수를 **선택**하기 위해 시도하는 유전 및 진화 알고리즘(Yao) -> Turner & Miller는 **이 전략을** 학습 중 단일 스케일링 **학습파라미터와 결합**함 -> 이 논문은 보다 강력한 “**adaptive activation function**” (적응형 활성화 함수)를 제안함

![Untitled.png](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper4/math_image.png?raw=true)

여기서 S는 힌지가 나타나는 횟수(하이퍼파라미터), a와 b는 학습파라미터 (a는 기울기, b는 힌지의 위치) (논문에서는 CIFAR-10데이터셋에 대해서 S가 5일때가 가장 좋음을 validation으로 밝히고 실험을 시작함)

연산량 (파라미터숫자)는 2SM (이때 M은 히든유닛의 전체 숫자) (전형적인 파라미터숫자보다 훨씬 적은 편이라고 소개됨.)

논문에서 실험은 소프트웨어 패키지 CAFFE (Jia et al., 2014)를 사용하여 수행되었는데, 조금 더 접근하기 쉽도록 colab상으로 구현해 봄.

[https://github.com/ForestAgostinelli/LearnedActivation-Functions-Source/tree/master](https://github.com/ForestAgostinelli/Learned-Activation-Functions-Source/tree/master)

선행연구2 - 정리한 활성화함수

![Untitled.png](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper4/3.png?raw=true)

활성화함수를 구분하는 주요 포인트 1 : 발산형 or 수렴형

주요포인트 2 : 곡선이 있느냐 없느냐

…

- 모델 & 데이터셋 소개

CODE 1,2에서 모델은 논문에서 제안한 구조를 그대로 사용 (그러나 dropout을 pooling 전후로 쓰던 구조를 pooling층 전 1번만 진행하고 비율도 각기 다르게 설정되어 있던 부분을 0.25로 모두 동일하게 설정함) (구조의 단순화를 위해)

CODE 3에서는

 여러 개의 합성곱 계층과 활성화 함수를 포함한 합성곱 신경망(CNN) 구조를 새로 작성함. (앞선 모델은 층을 늘리면 최종 피쳐맵이 1x1사이즈보다 작아지기 때문)

모델은 첫 번째 합성곱 계층을 통해 입력 이미지를 처리하고, 이어지는 합성곱 계층들은 동일한 채널 수를 유지하면서 특징을 추출합니다. 각 합성곱 계층 뒤에는 지정된 활성화 함수가 적용되며, 최종적으로 적응형 평균 풀링 계층과 완전 연결 계층을 통해 출력이 생성됩니다. 활성화 함수는 ReLU, Leaky ReLU 또는 APL(Affine Piecewise Linear) 중 하나로 선택할 수 있습니다. 이 모델은 다양한 층과 활성화 함수 조합의 성능을 평가하기 위해 사용되었습니다.

- 각자 실험 (methods)
- 코드 흐름대로 정리

CODE0, 1, 2, 3으로 구성됨

CODE0에서는 적절한 S값이 하이퍼파라미터임을 보임 (높은 S일수록 성능이 높아진다거나 그런 선형적 관계가 아님을 입증)

CODE1 논문과 비슷한 형태의 모델 아키텍처(3층)으로 활성화함수를 바꿔가며 accuracy와 loss를 시각화

CODE2

- Results
- Discussion
- Conclusion

### Reference

주논문 : Learning Activation Functions to Improve Deep Neural Networks (2015)

렐루 제안 논문 : What is the Best Multi-Stage Architectrue for object Recognition (2009)

리키렐루 제안 논문 : Deep Sparse Rectifier Neural Networks (2011)
