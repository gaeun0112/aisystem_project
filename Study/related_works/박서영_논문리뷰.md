# Activation Functions: Comparison of Trends in Practice and Research for Deep Learning (박서영)

[2018 | 2169회 인용]

### I. INTRODUCTION

- 논문 주제:  activation functions used in deep learning algorithms, their applications and the benefits of each function
- A common problem: how the gradient flows within the network,
    - owing to the fact that some gradients are sharp in specific directions and slow or even zero in some other directions thereby creating a problem for an optimal selection techniques of the learning parameters.
    - Vanishing & Exploding gradients
- New challenges: To select the right activation functions to perform in different domains from object classification, segmentation, scene description, machine translation, …
    - 언급된 도메인 → 레퍼런스 참고용!
        
        object recognition and classification
        
        speech recognition
        
        segmentation
        
        scene understanding and description
        
        machine translation
        
        test to speech systems
        
        cancer detection systems
        
        finger print detection
        
        weather forecast
        
        self-driving cars
        
- 선행 연구로 general regression & classification에 사용되는 AFs 비교한 것 있지만, 터키어로 작성되어 이해 어려움 (터키 학자가 아닌 사람들에게..)
- 본 연구: DL에 사용된 기존 AFs 적용 트렌드 요약
    - 섹션 1~4: DL, AF, AFs used in DL, application trends of AFs in deep architectures
    - 섹션 5: 함수에 대한 논의
    - 섹션 6: 결론, 향후 연구 방향 제시

### II. ACTIVATION FUNCTIONS

- Linear model
    - $f(x)=w^Tx+b$
    - output of the model: $y=(w_1x_1+w_2x_2+...+w_nx_n+b)$
        - linear by default
    - AF → convert linear outputs into non-linear output for further computation
        - Nonlinear AF($\alpha$) are required to convert
            
            $y=\alpha(w_1x_1+w_2x_2+...+w_nx_n+b)$
            
    - Derivative terms < 1 → 0으로 (vanish)
    - Derivative terms > 1 → inf로 (explode)
    - AFs는 이러한 gradient 값이 특정 limit을 갖게 함
    

### III. SUMMARY OF AFs

1. **Sigmoid Function** (logistic func, squashing func)
    - feedforward에 주로 사용되는 non-linear AF
    
    $$
    f(x)=\frac{1}{1+e^{-x}}
    $$
    
    - Output layer에 주로 사용 (확률 예측)
    - 이해하기 쉽고 주로 얕은 network에 사용됨
    - small random weight으로부터 초기화 할 때에는 피해야 함
    - 주요 문제
        - Gradient Vanishing
        - Gradient Saturation
        - Slow convergence
        - 중심이 0이 아님 (안정적인 학습 X)
        
        → 해결 위해 Tanh 제안됨
        
    1. **Hard Sigmoid Function**
        
        $$
        f(x)=clip(\frac{(x+1)}{2}, 0, 1)
        $$
        
        - Lesser computation cost
        - 이진분류 task에서 유망한 결과를 보임
    2. **Sigmoid-weighted Linear Units (SiLU)**
        
        $$
        a_k(s)=z_k\alpha(z_k)
        $$
        
        - RL based approximation function
        - s = input vector
        - $z_k$ = input to hidden units k
            
            $z_k=\sum w_{ik}s_i+b_k$
            
            - $b_k$ = bias
            - $w_{ik}$ = weight connecting to the hidden units k respectively
        - SiLU can only be used in the hidden layers of the DNN and only for RL based sys
        - outperformed the ReLU
    3. **Derivative of Sigmoid-Weighted Linear Units (dSiLU)**
        
        $$
        a_k(s)=\alpha(z_k)(1+z_k(1-\alpha(z_k)))
        $$
        
        - Gradient-descent learning updates for NN weight parameter
        - outperformed the standard Sigmoid
            
            ![스크린샷 2024-05-07 오후 4.32.07.png](Activation%20Functions%20Comparison%20of%20Trends%20in%20Pract%2049ac31f60b714e5b89300d8caadcc6bf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-05-07_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.32.07.png)
            
2. **Hyperbolic Tangent Function (Tanh)**
    
    $$
    f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
    $$
    
    - Smoother, zero-centred function (-1, 1)
    - 시그모이드보다 더 나은 성능을 보임
    - 기울기 소실 문제 해결 X
    - RNN for NLP, speech recognition task에 주로 사용
    1. **Hard Hyperbolic Function**
    
    $$
    f(x)=\begin{cases}
    -1, & \text{if } x<-1 \\
    x, & \text{if } -1= x\leq 1 \\
    1, & \text{if } x>1
    \end{cases}
    
    $$
    
    - 계산 효율성 굿
    - NLP에 성공적인 적용 (speed, acc improvement)
3. **Softmax Function**
    
    $$
    f(x_i)=\frac{exp(x_i)}{\sum_jexp(x_j)}
    $$
    
    - 확률 분포 계산 [0, 1], sum = 1
    - multi-class model의 각 클래스 별 확률 계산에 사용
    - DL의 거의 모든 output layer에 사용
    - Sigmoid는 이진분류, Softmax는 multi
4. **Softsign**
    
    $$
    f(x)=\frac{x}{|x|+1}
    $$
    
    - 회귀 문제에 주로 사용됨
    - speech sys에서 괜찮은 성능을 보임
5. **Rectified Linear Unit (ReLU) Function**
    
    $$
    f(x)=max(0, x)
    $$
    
    - SOTA에서 많이 사용되는 AF
    - Faster learning AF
    - 더 나은 성능, generalization을 보임
    - 분류, speech recognition task → Output unit에서 사용
    - 빠른 계산 보장 (단순계산~~)
    - 오버피팅 잘 됨 ㅠㅠ → 드롭아웃 사용
    - 종종 fragile, dead neurons 생김 → 해결 위해 LReLU 제안
    1. **Leaky ReLU (LReLU)**
        
        $$
        f(x)=\alpha x+x= \begin{cases}
        x, & \text{if } x>0 \\
        \alpha x, & \text{if } x\leq0
        \end{cases}
        
        $$
        
        - Dead neuron problem 해결 위해 알파 도입
        - Automatic speech recognition dataset에 실험됨
    2. **Parametric Rectified Linear Units (PReLU)**
        
        $$
        f(x_i)=\begin{cases}
        x_i, & \text{if } x_i>0 \\
        a_ix_i, & \text{if } x_i\leq 0
        \end{cases}
        
        $$
        
        - negative part → adaptively learned
        - $a_i$ is the negative slope controlling learnable parameter
        - large scale image recognition에서 ReLU보다 뛰어남
    3. **Randomized Leaky ReLU (RReLU)**
        
        $$
        f(x_i)=\begin{cases}
        x_{ji}, & \text{if } x_{ji}\geq0 \\
        a_{ji}x_{ji}, & \text{if } x_{ji}< 0
        \end{cases}
        
        $$
        
        - $a_i \sim U(l, u), l<u$  and $l, u \in [0, 1]$
        - tested on standard classification datasets and compared against the other variants of the ReLU AF
        - LReLU, RReLU and PReLU performs better than the ReLU on classification tasks
    4. **S-shaped ReLU (SReLU)**
        
        $$
        f(x)=\begin{cases}
        t_i^r+a^r_i(x_i-t^r_i), & x_i\geq t^r_i \\
        x_i, & t_i^r>x_i>t^l_i\\
        t^l_i+a^l_i(x_i-t^l_i), & x_i\leq t^l_i
        \end{cases}
        
        $$
        
        - SReLU was tested on some of the award-winning CNN architectures, the Network in Network architecture alongside GoogLeNet, for on image recognition tasks specifically CIFAR-10, ImageNet, and MNIST standard datasets, and it showed improved results, compared to the other AFs
6. **Softplus Function**
    
    $$
    f(x)=log(1+e^x)
    $$
    
    - Statistical application에 주로 적용
    - Speech recognition sys에 사용
    - ReLU, Sigmoid보다 높은 성능, 적은 epoch를 보였다!!
7. **Exponential Linear Units (ELUs)**
    
    $$
    f(x)=\begin{cases}
    x, & \text{if } {x>0} \\
    \alpha exp(x), & \text{if }x \leq0
    \end{cases}
    
    $$
    
    - 더 빠른 학습을 위해 제안
    - 기울기 소실 문제 완화
    - 5 레이어 이상의 구조에서 성능 굿
    - better generalization compared to ReLU and LReLU
    - 0 center 아님 → PELU 제안됨
    1. **Parametric Exponential Linear Unit (PELU)**
        
        $$
        f(x)=\begin{cases}
        cx, & \text{if } {x>0} \\
        \alpha exp(\frac{x}{b})-1, & \text{if }x \leq0
        \end{cases}
        
        $$
        
        - good option for applications that requires less bias shifts and vanishing gradients like the CNNs
    2. **Scaled Exponential Linear Units (SELU)**
        
        $$
        f(x)=\tau \begin{cases}
        x, & \text{if } {x>0} \\
        \alpha exp(x)-\alpha, & \text{if }x \leq0
        \end{cases}
        
        $$
        
        - not affected by vanishing and exploding gradient problems
        - classification tasks 성공적 적용
8. **Maxout Function**
    
    $$
    f(x)=max(w^T_1x+b_1, w^T_2x+b_2)
    $$
    
    - phone recognition applications에서 성공적 실험
    - computationally expensive
9. **Swish Function**
    
    $$
    f(x)=x\cdot sigmoid(x)=\frac{x}{1+e^{-x}}
    $$
    
    - hybrid AF
    - RL based automatic search technique 사용
    - smoothness → better optimization, generalization
    - simplicity, improved acc (기울기 소실X)
    - outperformed the ReLU on DL 분류 task
10. **ELiSH (Exponential linear Squashing)**
    
    $$
    f(x)=\begin{cases}
    \frac{x}{1+e^{-x}}, & {x\geq0} \\
    \frac{e^x-1}{1+e^{-x}}, & x<0
    \end{cases}
    
    $$
    
    - ImageNet에 성공적 적용 (CNN)
    1. **HardELiSH**
        
        $$
        f(x)=\begin{cases}
        x\times max(0, min(1, \frac{x+1}{2})), & {x\geq0} \\
        (e^x-1)\times max(0, min(1, \frac{x+1}{2})), & x<0
        \end{cases}
        
        $$
        
        - tested on ImageNet classification dataset
        
        ![스크린샷 2024-05-08 오전 11.35.47.png](Activation%20Functions%20Comparison%20of%20Trends%20in%20Pract%2049ac31f60b714e5b89300d8caadcc6bf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-05-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_11.35.47.png)
        

### IV. COMPARISON OF THE TRENDS IN AFs USED IN DL ARCHITECTURES

- Based on ImageNet (Image Large Scale Visual Recognition Challenge; ILSVRC) competition
    - SOTA architectures of DNN used for ILSVRC
        
        ![스크린샷 2024-05-07 오후 3.45.15.png](Activation%20Functions%20Comparison%20of%20Trends%20in%20Pract%2049ac31f60b714e5b89300d8caadcc6bf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-05-07_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_3.45.15.png)
        
    - more recent DL architectures → ReLU activation function
        - DenseNet, MobileNets, a mobile version of the convolutional networks, ResNeXt
    - The SeNet architecture is the current SOTA architecture for recognition tasks.
    

### V. DISCUSSIONS

- Linear units of AF are the most studied types of AF with rectified and exponential variants
- The ELU’s has been highlighted as a faster learning AF compared to their ReLU counterpart, and this assertion has been validated by Pedamonti, 2018, after an extensive comparison of some variants of the ELU and ReLU AF on the MNIST recognition dataset [82].
    - [https://arxiv.org/abs/1804.027](https://arxiv.org/abs/1804.02763) (MNIST)
        - The learning rate is faster in ELU and SELU compared to the ReLU and Leaky ReLU
        
        ![스크린샷 2024-05-08 오전 11.52.59.png](Activation%20Functions%20Comparison%20of%20Trends%20in%20Pract%2049ac31f60b714e5b89300d8caadcc6bf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-05-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_11.52.59.png)
        
        ![스크린샷 2024-05-08 오전 11.53.53.png](Activation%20Functions%20Comparison%20of%20Trends%20in%20Pract%2049ac31f60b714e5b89300d8caadcc6bf/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-05-08_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_11.53.53.png)
        
- There are other AFs that performs better than the ReLU, which has been the most consistent AF for DL applications since invention.
- The other variants of the ReLU, including LRELU, PReLU and RReLU performs better than the ReLU but some of these functions lack theoretical justifications to support their SOTA results.
- Furthermore, the parametric AFs have been a development in emerging applications where the AF was used as a learnable parameter from the dataset, thus looks to be a promising approach in the new functions developed most recently as observed in SReLU, PELU and PReLU.
- newer activation functions seem to outperform the older AFs like the ReLU, yet even the latest DL architectures rely on the ReLU function.
    - current practices depends on the tested and proven AFs
        
        → the newer activation functions are rarely used in practice
        

### VI. CONCLUSION

- there are other AFs that have not been discussed
- A future work would be to compare all these SOTA functions on the award-winning architectures, using standard datasets to observe if there would be improved performance results

### 제안..

- 최신 활성화함수와 ReLU, Softmax, Sigmoid(기존에 많이 사용되는 AFs) 비교해보기?
- 최신 활성화함수가 실전에서 잘 사용되지 않는 이유 찾아보기? (충분한 이론적 증명이란?)
- 최신 활성화함수에 대한 논문 찾아보기 → 구현? 직접 실험?