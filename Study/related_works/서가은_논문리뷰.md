# Is it Time to Swish? Comparing Deep Learnin Activation Functions Across NLP tasks(서가은)

## 1. Introduction

- 선형 분류기를 비선형으로 전환해주는 활성화 함수는 신경망에서 매우 중요한 요소이다.
- sigmoid와 tanh와 같은 서로 다른 활성화 함수들이 종종 이론적과 다르게 실제 훈련에서는 다양한 행동을 보이는 경우가 있다.
    - 예를 들어, sigmoid는 작은 기울기로 인하여 vanishing gradients를 발생시키고, 훈련에 적합하지 않은 모습을 보여주었다.
- 양수에서는 동일한 기울기를 유지하기 때문에 vanishing gradients를 덜 발생시키는 ReLU는 가장 유명한 활성화 함수로 자리잡았다.
    - ReLU의 다양한 변형이 있었으나, ReLU의 단순함을 따라잡지 못하는 문제와 모델과 데이터셋에 따라 안정적이지 못한 성능에 ReLU만큼 유명해지지는 못했다.
- 그러던 중, Searching for activaion functions에서 가장 좋은 성능을 내는 활성화 함수를 찾는 실험을 시도했다.
    - 해당 논문에서는 image classification task에서 CIFAR과 ImageNet 데이터에 몇가지 심층신경망 구조를 적용하여 활성화 함수들을 평가했다.
    - 그러나, task도 하나만 사용하였고 신경망 구조도 다양하게 사용하지 않았기 때문에 여전히 어떤 활성화 함수가 다른 task들과 모델들에서 가장 안정적으로 좋은 성능을 내는지에 대한 궁금증은 아직 풀리지 않았다.
- 때문에, 본 논문에서는 아래의 방법들로 이 부분을 채울 것이다.
    - 21개의 다른 활성화 함수들을 비교한다.
    - 3개의 유명한 NLP task들(문장 분류, 문서 분류, sequence tagging)을 수행한다.
    - 이 때, MLP, CNN, RNN과 같은 다른 신경망 구조들을 사용한다.
    - 모든 활성화 함수들의 top 성능과 평균 성능을 비교한다.
- 결과적으로, 비교적 유명하지 않았던 penalized tanh라는 활성화 함수가 다른 task들에서 가장 안정적이었고, 이 함수가 LSTM 셀 내부의 tanh와 sigmoid도 성공적으로 대체할 수 있음을 발견했다.

## 2. Theory

![Untitled](Is%20it%20Time%20to%20Swish%20Comparing%20Deep%20Learnin%20Activat%20176013e7fa514ed79a0be10816493353/Untitled.png)

- Searching for activaion functions에서 사용한 6개의 활성화 함수들(sigmoid, swish, maxsig, cosid, minsin, arctid, maxtanh) 외에도 21개의 활성화 함수들을 적용했다.
    - 기타 21개의 활성화 함수들
        - tanh, sin, relu, lrelu-0.01, lrelu-0.30, maxout-2, maxout-3, maxout-4, prelu, linear, elu, cube, penalized tanh, selu
        - penalized tanh?
            - 기존의 tanh에서 음수 부분에 페널티를 주는 활성화 함수
- 각 활성화 함수들의 특징
    
    ![Untitled](Is%20it%20Time%20to%20Swish%20Comparing%20Deep%20Learnin%20Activat%20176013e7fa514ed79a0be10816493353/Untitled%201.png)
    
    - sigmoid와 tangh, cube는 양의 극한으로 갈수록 exploding gradient가 발생하고, 음의 극한으로 갈수로 gradient vanishing이 발생한다.

## 3. Experiments

### 3.1. MLP & Sentence Classification

- Model
    - multi-layer perceptron(MLP)를 사용했다.
    - 모델의 input : 고정된 크기의 임베딩 벡터로 표현된 문장 혹은 짧은 텍스트.
        - 이 때, 조건의 다양성을 위해 임베딩 벡터는 Sent2Vec(600차원)와 InferSent(4096차원) 2가지를 적용했다.
    - 모델의 output : 분류 라벨
    
    ![Untitled](Is%20it%20Time%20to%20Swish%20Comparing%20Deep%20Learnin%20Activat%20176013e7fa514ed79a0be10816493353/Untitled%202.png)
    
    - $f$ : 활성화 함수
    - $x_0$ : input 표현
    - $y$ : output. 분류 task 에서 각 클래스에 해당할 확률.
- Data
    
    ![Untitled](Is%20it%20Time%20to%20Swish%20Comparing%20Deep%20Learnin%20Activat%20176013e7fa514ed79a0be10816493353/Untitled%203.png)
    
    - 조건의 다양성을 위해 전통적인 문장 분류 task인 MR, SUBJ, TREC에다가 문장 내에 특정 타입이 포함되었는지를 찾는 AM 데이터를 추가하여 실험했다.
    
    ![Untitled](Is%20it%20Time%20to%20Swish%20Comparing%20Deep%20Learnin%20Activat%20176013e7fa514ed79a0be10816493353/Untitled%204.png)
    
    - 또한, 조건의 다양성을 위해 랜덤으로 선택된 200개의 하이퍼파라미터들을 실험했다.
    - score는 최고점수와 평균 점수를 계산했는데, 최고 점수는 MLP가 잘 최적화되었을 때의 결과를 보여주고 평균 점수는 다양한 하이퍼파라미터 세팅들의 평균 점수를 나타낸다.
    - 그 외 세부 설정
        - patience 10 for early stopping, batch size 16, 100 epochs
- 결과
    
    ![Untitled](Is%20it%20Time%20to%20Swish%20Comparing%20Deep%20Learnin%20Activat%20176013e7fa514ed79a0be10816493353/Untitled%205.png)
    
    - best score에서 가장 높은 점수를 기록한 relu와 가장 낮은 점수를 기록한 linear의 점수 차가 2%밖에 되지 않는데, 이는 하이퍼 파라미터 탐색이 잘 이뤄지지 않는다면 활성화 함수의 선택이 문장 분류 task에서 크게 영향을 미치지 못한다는 분석을 할 수있다.
    - 또한, cube의 경우에는 best score와 mean score가 큰 차이가 났는데, 더 주의깊은 하이퍼 파라미터 탐색을 필요로 하는 위험한 활성화 함수임을 의미한다.

### 3.2 CNN & Document Classification

- 문서 분류 작업에서는 CNN 모델 사용.
- 데이터는 전형적인 문서 분류 task에 해당하는 NG와 R8을 사용.
- 실험 세팅
    - batch size 64, 50 epochs, 10 patience, 200 randomly hyper parameters
- 결과
    
    ![Untitled](Is%20it%20Time%20to%20Swish%20Comparing%20Deep%20Learnin%20Activat%20176013e7fa514ed79a0be10816493353/Untitled%206.png)
    

### 3.3 RNN & Sequence Tagging

- sequence tagging이란, input token w1, …, wK 시퀀스를 각각 라벨 시퀀스 y1, …, yK에 매핑하는 테스크로, POS tagging, chunking, NER, discourse parsing, argumentation mining task들을 포함한다.
- 논문에서는 TL-AM (argumentation mining)과 POS 데이터를 사용했다.
- 실험 세팅
    - batch size 32, 50 epochs, 5 patiences
- 결과
    
    ![Untitled](Is%20it%20Time%20to%20Swish%20Comparing%20Deep%20Learnin%20Activat%20176013e7fa514ed79a0be10816493353/Untitled%207.png)
    
    - 문장 분류에 비해 최고 성능과 최저 성능의 수치 차이가 크게(20% 정도) 난다는게 특징인데, 사실 POS tagging만 봤을 때는 차이가 크지 않으나 TL-AM 데이터의 차이 때문에 발생하는 현상이다.

## 4. Analysis & Discussion

- 총 17개의 모든 세부 실험들에서 각각의 활성화 함수들이 top 3 안에 드는 빈도를 계산.

![Untitled](Is%20it%20Time%20to%20Swish%20Comparing%20Deep%20Learnin%20Activat%20176013e7fa514ed79a0be10816493353/Untitled%208.png)

- mean에서 빈도가 높게 나온 활성화 함수들이 모두 범위가 제한된 함수들인 사실에서 범위가 제한된 경우에 보다 안정적으로 작동한다는 경향이 있다는 것을 알 수 있다.
- 또한 두 카테고리 모두에서 성능이 좋은 penalized tanh의 원점 주변의 기울기 형태를 통해 원점 주변에서 기울기가 가파른(뚱뚱한) cube가 왜 성능이 좋지 않은지를 설명할 수 있다.
- 하이퍼 파라미터의 영향
    - 하이퍼 파라미터들이 활성화 함수들에 얼마나 영향을 미치는지 직관을 가지기 위해 아래와 같이 사용된 하이퍼 파라미터들을 score 함수로 만들었다.
    
    ![Untitled](Is%20it%20Time%20to%20Swish%20Comparing%20Deep%20Learnin%20Activat%20176013e7fa514ed79a0be10816493353/Untitled%209.png)
    
    - y : test set에서의 score
    - $n_l$ : 네트워크의 층의 수
    - $d$ : dropout 파라미터
    - 로그는 스케일링을 위해 사용.
    - 모든 함수들에 대해 계산 결과, 아래와 같은 결과를 얻었다.
        - 모든 모델들이 평균적으로 더 적은 은닉층일 때 더 좋은 성능을 냈다. 단, swish만 더 많은 은닉층에 강건했다.
        - 문장 분류 task에서는 sin과 maxout 함수들이 특별히 은닉층의 증가에 강건했다.
        - penalized tanh는 기울기가 쉽게 포화되는(0에 가까워지는) 함수이고, sin은 심지어 진동하는 함수이기 때문에, 기울기를 보존하는 것(ex: relu처럼 1로)은 심층신경망의 성공적인 학습에 필수 요건이 아니라고 결론지었다.

## 5. Concluding remarks

- 논문에서는 아래과 같은 사실들을 발견했다.
    - 기존 연구에서 best performer였던 swish는 이번 실험에서도 각각의 task들에서 좋은 성능을 보였지만, 불안정한 모습을 보였다.
    - penalized tanh 함수는 이러한 관점에서 하이퍼파라미터의 변화와 같은 조건에서도 더 안정적으로 좋은 성능을 보였다.
    - 이러한 결과는 하이퍼파라미터 최적화가 힘들 때 활성화 함수의 선택을 도와줄 수 있다.