# Review and Comparison of Commonly Used Activation
Functions for Deep Neural Networks(안민우)

활성화 함수에 대한 비교적 최근의 연구들을 다룸

연구 목표 → 활성화 함수 비교를 통해 신경망의 성능을 최적화하는 것.

1. Introduction
    - 활성화 함수의 역할과 중요성
        - 신경망에 비선형성 주입
        - 입력 신호를 다음 레이어로 전달하기 전에 활성화

1. Activation Functions Overview(일부 함수만 기술)
    
    : 주요 활성화 함수들의 수학적 정의와 장단점, 신경망에서의 작동 원리 등 기술
    
    - ReLU(Rectified Linear Unit)
        
        ![Untitled](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper3/Untitled.png?raw=true)
        
        ![Untitled](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper3/Untitled%201.png?raw=true)
        
        - 입력이 양수인 경우에는 그 값을 그대로 출력하고, 음수인 경우에는 0으로 출력하는 간단한 비선형 함수
        - 계산이 간단하고, 그레이디언트 소실 문제를 완화
        - 수렴 속도가 빠르고 효율적으로 학습
        - 입력값이 음수인 경우에는 그레이디언트가 0이 되어 학습이 멈추는 문제(Dying ReLU Problem)
        
    - Sigmoid
        
        ![Untitled](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper3/Untitled%202.png?raw=true)
        
        ![Untitled](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper3/Untitled%203.png?raw=true)
        
        - 입력을 (0, 1) 사이의 값으로 압축시키는 S 형태의 곡선을 가지는 함수
        - 입력이 큰 양수 또는 음수일 때 gradient vanishing 발생 가능
        - 이진 분류 문제에서 출력층에 사용되며, 출력값을 확률로 해석할 수 있어 확률적 출력을 필요로 하는 경우에 유용
        
    - Swish
        
        ![Untitled](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper3/Untitled%204.png?raw=true)
        
        ![Untitled](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper3/Untitled%205.png?raw=true)
        
        ![Untitled](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper3/Untitled%206.png?raw=true)
        
        - (위의 그림과 수식) basic swish를 가능하게 하기 위해 0이 아닌 파라미터 베타가 제안되기도 했는데, 베타 값의 변화에 따른 그래프 시각화
        - Mish 함수와 함께 최근에 제안된 비선형 활성화 함수 중 하나
        - 입력값에 대한 비선형 변환을 수행하며, 함수의 모양은 S자 형태
        - ReLU와 유사하게 계산이 간단하면서도, gradient vanishing 해결
        - Swish 함수가 ReLU와 비교하여 더 나은 성능을 보이는 경우도 있음
        
    - Leaky ReLU
        
        ![Untitled](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper3/Untitled%207.png?raw=true)
        
        ![Untitled](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper3/Untitled%208.png?raw=true)
        
        - 입력값이 음수일 때 작은 기울기를 갖는 ReLU 함수의 변형
        - Dying ReLU Problem을 완화시키기 위해 도입
        - 입력값이 음수일 때 작은 기울기를 갖기 때문에, gradient vanishing을 완화하고, 학습을 안정화

1. Experiments & Conclusion
    - CIFAR-10으로 실험 진행
    - 모델은 CNN, MLP등 사용
    - 역전파 알고리즘으로 loss function을 최소화하는 방향으로 모델링
    - 함수 평가 지표 : Classification Accuracy, average training time, time to classify
    
    ![Untitled](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper3/Untitled%209.png?raw=true)
    

- accuracy 측면에서는, ReLU와 leaky ReLU의 성능이 가장 높음 + Dying ReLU Problem에 대한 시사점(전체 성능 면에서는 저하 x)

![Untitled](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper3/Untitled%2010.png?raw=true)

- 모델 훈련 시간 측면에서도 ReLU가 선두

![Untitled](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper3/Untitled%2011.png?raw=true)

- task 수행 시간에서도 ReLU

 ——————————————————————————————

< 결론 정리 >

![Untitled](https://github.com/gaeun0112/aisystem_project/blob/main/image/paper3/Untitled%2012.png?raw=true)
